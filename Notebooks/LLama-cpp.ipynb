{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd6cff27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your query\n",
      "Query: How LLM's Works\n",
      "AI response:\n",
      "Okay, let's break down the problem into sub-problems and find an optimal solution:\n",
      "\n",
      "1. **Understanding what LLM stands for**: \n",
      "   - I know that \"LLM\" likely stands for Language Model.\n",
      "\n",
      "2. **Identify key components of a language model**:\n",
      "   - A language model is typically composed of several parts, including its architecture (e.g., transformer-based architectures), training process, and various features or capabilities it possesses.\n",
      "  \n",
      "3. **Assess the structure and typical functionality within an LLM**:\n",
      "   \n",
      "    An example could involve: \n",
      "       a) The model uses neural networks to understand language by processing vast amounts of text data as input.\n",
      "       b) Training is done via algorithms like gradient descent, optimizing parameters such as weights or biases over time using labeled training samples.\n",
      "   c) Features may include abilities for tasks beyond simple generation (such as understanding context and style). \n",
      "   d) Outputs typically range from basic language production to more complex linguistic insights.\n",
      "\n",
      "4. **Expanding the answer by adding details**:\n",
      "\n",
      "    The Language Models rely on:\n",
      "       i) Data input: text, speech.\n",
      "       ii) Algorithm design: using neural networks with layers for each level of abstraction or understanding.\n",
      "       iii) Training methods: aiming at optimization and learning to generalize well from examples.\n",
      "\n",
      "   This LLM can also be designed through specific architectures like GPT-3 (Generative Pre-trained Transformer 3), BERT, or T5 which help it generate human-like text in a wide range of applications including translation, question answering etc.\n",
      "   \n",
      "By focusing on these aspects and understanding the process within each sub-problem step-by-step while ensuring my answer remains relevant to your query about LLMs' workings.------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Enter your query\n",
      "Query: exit\n",
      "AI response:\n",
      "You've requested to exit, but without a prompt or context about the specific system you're using (e.g., operating systems like Windows, Linux), it's impossible for me to provide an accurate and helpful response regarding your request. Please let me know if this was in reference to one of our software platforms so I can offer more relevant information.------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# pip install llama-cpp-python \n",
    "# huggingface-cli download Qwen/Qwen2.5-1.5B-Instruct-GGUF qwen2.5-1.5b-instruct-q5_k_m.gguf --local-dir . --local-dir-use-symlinks False\n",
    "\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# Initialize the Llama model\n",
    "llm = Llama(\n",
    "    model_path=\"../LLM-Quantize-Model/qwen2.5-1.5b-instruct-q5_k_m.gguf\",  # Ensure model is optimized for CPU\n",
    "    # n_gpu_layers=None,  # Comment out GPU-related parameters since you're using CPU\n",
    "    temperature=0.7,  # Lower temperature for less randomness, faster processing\n",
    "    top_p=0.9,        # More focused sampling (decreases computational complexity)\n",
    "    n_ctx=32768,       # Reduce context window to save memory and increase speed\n",
    "    max_tokens=8192,  # Reduce output length for quicker responses\n",
    "    repeat_penalty=1.1,  # Slightly reduce the penalty for repetition\n",
    "    stop=['<|im_end|>'],  # Keep the stop token to control output termination\n",
    "    verbose=False,    # Disable verbose logging for efficiency\n",
    ")\n",
    "\n",
    "user_input = \"\"\n",
    "\n",
    "while user_input != \"exit\":\n",
    "    # Define the user input (prompt)\n",
    "    print(\"Enter your query\")\n",
    "    user_input = input()\n",
    "    print(\"Query:\", user_input)\n",
    "    # Prepare the message\n",
    "    messages = [{\"role\": \"system\", \n",
    "                \"content\": \"\"\"You are working as helpful assistant and your job is to understand aim, goal and objective of user query, use chain of thought to break down problem into subproblem, find best optimize solution and while combinting sub-solution check answer relevency with query and give response,\n",
    "\n",
    "                Instruction:\n",
    "                    ** Do not try to short the answer, explain as much as you can without thinking token count **\n",
    "\n",
    "                \"\"\"},\n",
    "                {\"role\": \"user\", \"content\": user_input}]\n",
    "\n",
    "    # Generate a response from the model with streaming enabled\n",
    "    response = llm.create_chat_completion(\n",
    "        messages=messages,\n",
    "        temperature=1.2,\n",
    "        repeat_penalty=1.178,\n",
    "        stop=['<|im_end|>'],\n",
    "        max_tokens=1500,\n",
    "        stream=True  # Enable streaming\n",
    "    )\n",
    "\n",
    "    print(\"AI response:\")\n",
    "    # Stream the response and print each chunk as it comes\n",
    "    response_content = \"\"\n",
    "    for chunk in response:\n",
    "        try:\n",
    "            # Append the content of each chunk to the response\n",
    "            if \"choices\" in chunk and \"delta\" in chunk[\"choices\"][0] and \"content\" in chunk[\"choices\"][0][\"delta\"]:\n",
    "                content = chunk[\"choices\"][0][\"delta\"][\"content\"]\n",
    "                response_content += content\n",
    "                print(content, end=\"\", flush=True)  # Print the response in one line\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "    print(\"-\"*150)  # New line after response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a3c7c3",
   "metadata": {},
   "source": [
    "### Vision Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477faeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install llama-cpp-python \n",
    "# huggingface-cli download Qwen/Qwen2.5-1.5B-Instruct-GGUF qwen2.5-1.5b-instruct-q5_k_m.gguf --local-dir . --local-dir-use-symlinks False\n",
    "\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# Initialize the Llama model\n",
    "try:\n",
    "    llm = Llama(\n",
    "        model_path=\"../LLM-Quantize-Model/Qwen-vlm-3B.gguf\",  # Ensure model is optimized for CPU\n",
    "        n_gpu_layers=4,  # Specify GPU layers or modify based on your setup\n",
    "        temperature=1.1,\n",
    "        top_p=0.5,\n",
    "        n_ctx=32768,\n",
    "        max_tokens=1500,\n",
    "        repeat_penalty=1.178,\n",
    "        stop=['<|im_end|>'],\n",
    "        verbose=False\n",
    "    )\n",
    "except Exception as e: print(e)\n",
    "\n",
    "# Define the user input (prompt)\n",
    "user_input = \"Explain the importance of AI in education.\"\n",
    "\n",
    "# Prepare the message\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Generate a response from the model\n",
    "response = llm.create_chat_completion(\n",
    "    messages=messages,\n",
    "    temperature=1.2,\n",
    "    repeat_penalty=1.178,\n",
    "    stop=['<|im_end|>'],\n",
    "    max_tokens=1500)\n",
    "\n",
    "# Extract and print the response in one line\n",
    "response_content = response['choices'][0]['message']['content']\n",
    "print(response_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1551fa6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLPRnD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
