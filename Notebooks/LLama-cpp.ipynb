{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd46c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd6cff27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your query\n",
      "Query: Explain LLMs\n",
      "AI response:\n",
      "Large Language Models (LLMs) are artificial intelligence models that can generate human-like responses to a wide range of questions and prompts. These models use deep learning algorithms, such as transformers or neural networks, to analyze vast amounts of text data.\n",
      "They work by processing large datasets containing language examples, which the model uses to learn patterns in natural language. This enables LLMs to understand context, grammar, syntax, semantics, and more when generating responses.\n",
      "\n",
      "LLMs are designed with flexibility and adaptability in mind. They can be trained on diverse corpora or tasks that involve understanding human dialogue, such as writing assistant software for writers.\n",
      "The complexity of the model also allows it to process information from different sources; for example, news articles, social media posts, chat logs - LLMs are capable of generating detailed summaries and reports based on these inputs.------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Enter your query\n",
      "Query: exit\n",
      "AI response:\n",
      "Thank you for your understanding, but it seems we may have misunderstood each other. Could you please provide more information or clarify your question? I am here to help answer any questions or assist with tasks as best as possible.------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# pip install llama-cpp-python \n",
    "# huggingface-cli download Qwen/Qwen2.5-1.5B-Instruct-GGUF qwen2.5-1.5b-instruct-q5_k_m.gguf --local-dir . --local-dir-use-symlinks False\n",
    "\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# Initialize the Llama model\n",
    "llm = Llama(\n",
    "    model_path=\"../LLM-Quantize-Model/qwen2.5-1.5b-instruct-q5_k_m.gguf\",  # Ensure model is optimized for CPU\n",
    "    # n_gpu_layers=None,  # Comment out GPU-related parameters since you're using CPU\n",
    "    temperature=0.7,  # Lower temperature for less randomness, faster processing\n",
    "    top_p=0.9,        # More focused sampling (decreases computational complexity)\n",
    "    n_ctx=32768,       # Reduce context window to save memory and increase speed\n",
    "    max_tokens=8192,  # Reduce output length for quicker responses\n",
    "    repeat_penalty=1.1,  # Slightly reduce the penalty for repetition\n",
    "    stop=['<|im_end|>'],  # Keep the stop token to control output termination\n",
    "    verbose=False,    # Disable verbose logging for efficiency\n",
    ")\n",
    "\n",
    "user_input = \"\"\n",
    "\n",
    "while user_input != \"exit\":\n",
    "    # Define the user input (prompt)\n",
    "    print(\"Enter your query\")\n",
    "    user_input = input()\n",
    "    print(\"Query:\", user_input)\n",
    "    # Prepare the message\n",
    "    messages = [{\"role\": \"system\", \n",
    "                \"content\": \"\"\"You are working as helpful assistant and your job is to understand aim, goal and objective of user query, use chain of thought to break down problem into subproblem, find best optimize solution and while combinting sub-solution check answer relevency with query and give response,\n",
    "\n",
    "                Instruction:\n",
    "                    ** Do not try to short the answer, explain as much as you can without thinking token count **\n",
    "\n",
    "                \"\"\"},\n",
    "                {\"role\": \"user\", \"content\": user_input}]\n",
    "\n",
    "    # Generate a response from the model with streaming enabled\n",
    "    response = llm.create_chat_completion(\n",
    "        messages=messages,\n",
    "        temperature=1.2,\n",
    "        repeat_penalty=1.178,\n",
    "        stop=['<|im_end|>'],\n",
    "        max_tokens=1500,\n",
    "        stream=True  # Enable streaming\n",
    "    )\n",
    "\n",
    "    print(\"AI response:\")\n",
    "    # Stream the response and print each chunk as it comes\n",
    "    response_content = \"\"\n",
    "    for chunk in response:\n",
    "        try:\n",
    "            # Append the content of each chunk to the response\n",
    "            if \"choices\" in chunk and \"delta\" in chunk[\"choices\"][0] and \"content\" in chunk[\"choices\"][0][\"delta\"]:\n",
    "                content = chunk[\"choices\"][0][\"delta\"][\"content\"]\n",
    "                response_content += content\n",
    "                print(content, end=\"\", flush=True)  # Print the response in one line\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "    print(\"-\"*150)  # New line after response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a3c7c3",
   "metadata": {},
   "source": [
    "### Vision Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477faeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install llama-cpp-python \n",
    "# huggingface-cli download Qwen/Qwen2.5-1.5B-Instruct-GGUF qwen2.5-1.5b-instruct-q5_k_m.gguf --local-dir . --local-dir-use-symlinks False\n",
    "\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# Initialize the Llama model\n",
    "llm = Llama(\n",
    "    model_path=\"../LLM-Quantize-Model/Qwen2-VL-2B-Q8_0.gguf\",  # Ensure model is optimized for CPU\n",
    "    # n_gpu_layers=4,  # Specify GPU layers or modify based on your setup\n",
    "    temperature=1.1,\n",
    "    top_p=0.5,\n",
    "    n_ctx=32768,\n",
    "    max_tokens=1500,\n",
    "    repeat_penalty=1.178,\n",
    "    stop=['<|im_end|>'],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Define the user input (prompt)\n",
    "user_input = \"Explain the importance of AI in education.\"\n",
    "\n",
    "# Prepare the message\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Generate a response from the model\n",
    "response = llm.create_chat_completion(\n",
    "    messages=messages,\n",
    "    temperature=1.2,\n",
    "    repeat_penalty=1.178,\n",
    "    stop=['<|im_end|>'],\n",
    "    max_tokens=1500)\n",
    "\n",
    "# Extract and print the response in one line\n",
    "response_content = response['choices'][0]['message']['content']\n",
    "print(response_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763be9f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLPRnD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
